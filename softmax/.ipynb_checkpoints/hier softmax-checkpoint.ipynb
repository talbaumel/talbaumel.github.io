{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling-Up Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with sequence to sequence models let’s discuss  how to scale them to real world problems. In the previous example we used a very small vocabulary (4 symbols $“a,b,c,d”$)  but when dealing with tasks such as automatic machine translation our vocabulary can contain thousands of unique words, that means that the last layer of our network must be the dimension as the size of the vocabulary and $softmax$ must be applied on an extremely long vector. When training a network we cannot afford such a costly operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's why hierarchical softmax was invented, it can increase softmax time by up to $log(n)$ without adding compromising too much of the network reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Softmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function, or normalized exponential is used to transform a $n$ dimensional vector into a probabilty over $n$ classes, formaly it is defined as: $P(class=i | x) = \\dfrac{e^{x_i}}{\\sum_{j=1}^ne^{x_j}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy code for SoftMax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the softmax we need to $O(n)$ time. Let's see how can we reduce it to $O(log(n))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hierarchical Softmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hierarchical softmax function first arranges the output vocabulary in an hierarchical tree (we will discuss later how this tree can be constructed). Once we constructed the tree we can use softmax to select each branch of the tree. For example, given the following hierarchy and the value of the last hidden layer is $V$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/HierSoftMax.jpg\" width=\"25%\" height=\"25%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilty of the network to predict \"D\" is (the probabilty of choosing the 2nd branch in $B_1$)*(the provavilty of choosing the 1st branch on $B_3$). More formally:  $P(class=D | V) = softmax(V \\times W_{B_1})_1*softmax(V \\times W_{B_3})_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what is hierarchical softmax we can start implementing it. We will create an hier_softmax that will recieve an output hierarchy and will be able to calculate of the probabilty of an output given $V$ and to generate to most likely output given a vector $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will define some auxiliary functions to handle trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from copy import copy\n",
    "\n",
    "def tag_tree(tree, tag):\n",
    "    for node in tree:\n",
    "        if type(node) == list:\n",
    "            tag += 1\n",
    "            tag_tree(tree, tag)\n",
    "def _get_subtrees(tree):\n",
    "    yield tree\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == list:\n",
    "            for x in _get_subtrees(subtree):\n",
    "                yield x\n",
    "\n",
    "\n",
    "# Returns pairs of paths and leafves of a tree\n",
    "def _get_leaves_paths(tree):\n",
    "    for i, subtree in enumerate(tree):\n",
    "        if type(subtree) == list:\n",
    "            for path, value in _get_leaves_paths(subtree):\n",
    "                yield [i] + path, value\n",
    "        else:\n",
    "            yield [i], subtree\n",
    "\n",
    "#memoization for _count_nodes functions\n",
    "_count_nodes_dict = {}\n",
    "# Returns the number of nodes in a tree (not including root)\n",
    "def _count_nodes(tree):\n",
    "    if id(_count_nodes_dict) in _count_nodes_dict:\n",
    "        return _count_nodes_dict[id(_count_nodes_dict)]\n",
    "    size = 0\n",
    "    for node in tree:\n",
    "        if type(node) == list:\n",
    "            size += 1 + _count_nodes(node)\n",
    "    _count_nodes_dict[id(_count_nodes_dict)] = size\n",
    "    return size\n",
    "\n",
    "\n",
    "# Returns all the nodes in a path\n",
    "def _get_nodes(tree, path):\n",
    "    next_node = 0\n",
    "    nodes = []\n",
    "    for decision in path:\n",
    "        nodes.append(next_node)\n",
    "        next_node += 1 + _count_nodes(tree[:decision])\n",
    "        tree = tree[decision]\n",
    "    return nodes\n",
    "\n",
    "\n",
    "# turns a list to a binary tree\n",
    "def random_binary_full_tree(outputs):\n",
    "    outputs = copy(outputs)\n",
    "    shuffle(outputs)\n",
    "\n",
    "    while len(outputs) > 2:\n",
    "        temp_outputs = []\n",
    "        for i in range(0, len(outputs), 2):\n",
    "            if len(outputs) - (i+1) > 0:\n",
    "                temp_outputs.append([outputs[i], outputs[i+1]])\n",
    "            else:\n",
    "                temp_outputs.append(outputs[i])\n",
    "        outputs = temp_outputs\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the auxiliary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our tree: [[[[7, 5], [4, 8]], [[3, 2], [6, 1]]], [0, 9]]\n",
      "All subtrees:\n",
      "\t[[[[7, 5], [4, 8]], [[3, 2], [6, 1]]], [0, 9]]\n",
      "\t[[[7, 5], [4, 8]], [[3, 2], [6, 1]]]\n",
      "\t[[7, 5], [4, 8]]\n",
      "\t[7, 5]\n",
      "\t[4, 8]\n",
      "\t[[3, 2], [6, 1]]\n",
      "\t[3, 2]\n",
      "\t[6, 1]\n",
      "\t[0, 9]\n",
      "All paths and leaves:\n",
      "\t([0, 0, 0, 0], 7)\n",
      "\t([0, 0, 0, 1], 5)\n",
      "\t([0, 0, 1, 0], 4)\n",
      "\t([0, 0, 1, 1], 8)\n",
      "\t([0, 1, 0, 0], 3)\n",
      "\t([0, 1, 0, 1], 2)\n",
      "\t([0, 1, 1, 0], 6)\n",
      "\t([0, 1, 1, 1], 1)\n",
      "\t([1, 0], 0)\n",
      "\t([1, 1], 9)\n",
      "Number of nodes in the tree: 14\n",
      "all nodes in path [0, 0, 0, 0]:\n",
      "\t0\n",
      "\t15\n",
      "\t30\n",
      "\t45\n"
     ]
    }
   ],
   "source": [
    "tree = random_binary_full_tree(range(10))\n",
    "print 'Our tree:',tree\n",
    "\n",
    "print 'All subtrees:'\n",
    "for subtree in _get_subtrees(tree):\n",
    "    print '\\t',subtree\n",
    "\n",
    "print 'All paths and leaves:'\n",
    "for subtree in _get_leaves_paths(tree):\n",
    "    print '\\t',subtree\n",
    "    \n",
    "print 'Number of nodes in the tree:',_count_nodes(tree)\n",
    "\n",
    "print 'all nodes in path [0, 0, 0, 0]:'\n",
    "for nodes in _get_nodes(tree, [0, 0, 0, 0]):\n",
    "    print '\\t',nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hier softmax class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pycnn as pc\n",
    "\n",
    "class hier_softmax:\n",
    "    def __init__(self, tree, contex_size, model):\n",
    "        _count_nodes_dict = {}\n",
    "        \n",
    "        #create a weight matrix and bias vector for each node in the tree\n",
    "        for i, subtree in enumerate(_get_subtrees(tree)):\n",
    "            model.add_parameters(\"softmax_node_\"+str(i)+\"_w\", (len(subtree), contex_size))\n",
    "            model.add_parameters(\"softmax_node_\" + str(i) + \"_b\", len(subtree))\n",
    "        \n",
    "        #create a dictionary from each value to its path\n",
    "        value_to_path_and_nodes_dict = {}\n",
    "        for path, value in _get_leaves_paths(tree):\n",
    "            nodes = _get_nodes(tree, path)\n",
    "            value_to_path_and_nodes_dict[data.char2int[value]] = path, nodes\n",
    "        self.value_to_path_and_nodes_dict = value_to_path_and_nodes_dict\n",
    "        self.model = model\n",
    "        self.tree = tree\n",
    "    \n",
    "    #get the loss on a given value (for training)\n",
    "    def get_loss(self, context, value):\n",
    "        loss = []\n",
    "        path, nodes = self.value_to_path_and_nodes_dict[value]\n",
    "        for p, n in zip(path, nodes):\n",
    "            w = pc.parameter(self.model[\"softmax_node_\"+str(n)+\"_w\"])\n",
    "            b = pc.parameter(self.model[\"softmax_node_\" + str(n) + \"_b\"])\n",
    "            probs = pc.softmax(w*context+b)\n",
    "            loss.append(-pc.log(pc.pick(probs, p)))\n",
    "        return pc.esum(loss)\n",
    "\n",
    "    #get the most likely\n",
    "    def generate(self, context):\n",
    "        best_value = None\n",
    "        best_loss = float(100000)\n",
    "        for value in self.value_to_path_and_nodes_dict:\n",
    "            loss = self.get_loss(context, value)\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_value = value\n",
    "        return best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the performance improvement we can get from the hier_softmax. Again, we will learn the reverse function, but this time on a big vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('203 147 126 52', '52 126 147 203')\n"
     ]
    }
   ],
   "source": [
    "from random import choice, randrange\n",
    "import data\n",
    "\n",
    "data.set_vocab_size(1000)\n",
    "\n",
    "print data.sample_model(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "MAX_STRING_LEN = 5\n",
    "\n",
    "train_set = [data.sample_model(1, MAX_STRING_LEN) for _ in xrange(3000)]\n",
    "val_set = [data.sample_model(1, MAX_STRING_LEN) for _ in xrange(50)]\n",
    "\n",
    "def train(network, train_set, val_set, epochs = 20):\n",
    "    def get_val_set_loss(network, val_set):\n",
    "        loss = [network.get_loss(input_string, output_string).value() for input_string, output_string in val_set]\n",
    "        return sum(loss)\n",
    "    \n",
    "    train_set = train_set*epochs\n",
    "    trainer = pc.SimpleSGDTrainer(network.model)\n",
    "    start = datetime.now()\n",
    "    for i, training_example in enumerate(train_set):\n",
    "        input_string, output_string = training_example\n",
    "        \n",
    "        loss = network.get_loss(input_string, output_string)\n",
    "        loss_value = loss.value()\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "    print datetime.now()-start\n",
    "        \n",
    "\n",
    "    print 'loss on validation set:', get_val_set_loss(network, val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a large vocab data we can measure the training time of the attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from models import AttentionNetwork\n",
    "\n",
    "ENC_RNN_NUM_OF_LAYERS = 1\n",
    "DEC_RNN_NUM_OF_LAYERS = 1\n",
    "EMBEDDINGS_SIZE = 200\n",
    "ENC_STATE_SIZE = 210\n",
    "DEC_STATE_SIZE = 210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "att = AttentionNetwork(ENC_RNN_NUM_OF_LAYERS, DEC_RNN_NUM_OF_LAYERS, EMBEDDINGS_SIZE, ENC_STATE_SIZE, DEC_STATE_SIZE)\n",
    "\n",
    "#train(att, train_set, val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add hierarchical softmax to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_tree = random_binary_full_tree(data.characters)\n",
    "\n",
    "RNN_BUILDER = pc.LSTMBuilder\n",
    "class AttentionNetworkWithHierSoftmax(AttentionNetwork):\n",
    "    def __init__(self, enc_layers, dec_layers, embeddings_size, enc_state_size, dec_state_size, tree):\n",
    "        self.model = pc.Model()\n",
    "\n",
    "        # the embedding paramaters\n",
    "        self.model.add_lookup_parameters(\"lookup\", (data.VOCAB_SIZE, embeddings_size))\n",
    "\n",
    "        # the rnns\n",
    "        self.ENC_RNN = RNN_BUILDER(enc_layers, embeddings_size, enc_state_size, self.model)\n",
    "        self.DEC_RNN = RNN_BUILDER(dec_layers, enc_state_size, dec_state_size, self.model)\n",
    "        \n",
    "        # attention weights\n",
    "        self.model.add_parameters(\"attention_w1\", (enc_state_size, enc_state_size))\n",
    "        self.model.add_parameters(\"attention_w2\", (enc_state_size, dec_state_size))\n",
    "        self.model.add_parameters(\"attention_v\", (1, enc_state_size))\n",
    "\n",
    "        self.enc_state_size = enc_state_size\n",
    "        \n",
    "        self.hier_softmax = hier_softmax(tree, dec_state_size, self.model)\n",
    "    \n",
    "    def _get_probs(self, rnn_output, output_char):\n",
    "        return self.hier_softmax.get_loss(rnn_output, output_char)\n",
    "    \n",
    "    def _predict(self, rnn_output):\n",
    "        return self.self.hier_softmax.generate(rnn_output)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "att = AttentionNetworkWithHierSoftmax(\n",
    "    ENC_RNN_NUM_OF_LAYERS, DEC_RNN_NUM_OF_LAYERS, EMBEDDINGS_SIZE, ENC_STATE_SIZE, DEC_STATE_SIZE, output_tree)\n",
    "\n",
    "#train(att, train_set, val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has we can see the hierarchical softmax shaved almost 12.5% off our training time. On real world data a vocabulary of 1,000 words is considered tiny so the gain should be considerably higher. Other difference from real data is that the training examples are not uniformly distributed and external information (such as wordnet) can further improve the hier softmax performance (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.8829&rep=rep1&type=pdf#page=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.041189\n",
      "loss on validation set: 83.8740997314\n",
      "0:00:00.064382\n",
      "loss on validation set: 57.4016151428\n"
     ]
    }
   ],
   "source": [
    "data.set_vocab_size(100000)\n",
    "\n",
    "train_set = [data.sample_model(1, MAX_STRING_LEN) for _ in xrange(1)]\n",
    "val_set = [data.sample_model(1, MAX_STRING_LEN) for _ in xrange(1)]\n",
    "\n",
    "output_tree = random_binary_full_tree(data.characters)\n",
    "\n",
    "\n",
    "ENC_RNN_NUM_OF_LAYERS = 1\n",
    "DEC_RNN_NUM_OF_LAYERS = 1\n",
    "EMBEDDINGS_SIZE = 10\n",
    "ENC_STATE_SIZE = 10\n",
    "DEC_STATE_SIZE = 100\n",
    "\n",
    "\n",
    "att = AttentionNetworkWithHierSoftmax(\n",
    "    ENC_RNN_NUM_OF_LAYERS, DEC_RNN_NUM_OF_LAYERS, EMBEDDINGS_SIZE, ENC_STATE_SIZE, DEC_STATE_SIZE, output_tree)\n",
    "\n",
    "train(att, train_set, val_set, 1)\n",
    "\n",
    "att = AttentionNetwork(\n",
    "    ENC_RNN_NUM_OF_LAYERS, DEC_RNN_NUM_OF_LAYERS, EMBEDDINGS_SIZE, ENC_STATE_SIZE, DEC_STATE_SIZE)\n",
    "\n",
    "train(att, train_set, val_set, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "About me: https://www.cs.bgu.ac.il/~talbau/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
